{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yEP9Ygyy4RtD",
        "aD-L1elQ4dVm",
        "wcm_Fg6BmnBm",
        "yw0tlqohpUts",
        "PsPjmAhyrIO7",
        "rsbThMtatJpM",
        "tUDv7vxGGjvI",
        "D4clNGJNtdeC",
        "3fLLSLz94XKJ",
        "ySb5mSHS4cDc",
        "xy_qwWW-DN7t",
        "xY2BNST-G7qe"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhibarSumit10/Online-Retail-Customer-Segmentation/blob/main/Online_Retail_Customer_Segmentation_Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOGC-qoyhJeX"
      },
      "source": [
        "# <b><u> Project Title : Extraction/identification of major topics & themes discussed in news articles. </u></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y06xIdG26kRF"
      },
      "source": [
        "## <b> Problem Description </b>\n",
        "\n",
        "### In this project, your task is to identify major customer segments on a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlLxAtlziMbP"
      },
      "source": [
        "## <b> Data Description </b>\n",
        "\n",
        "### <b>Attribute Information: </b>\n",
        "\n",
        "* ### InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
        "* ### StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
        "* ### Description: Product (item) name. Nominal.\n",
        "* ### Quantity: The quantities of each product (item) per transaction. Numeric.\n",
        "* ### InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated.\n",
        "* ### UnitPrice: Unit price. Numeric, Product price per unit in sterling.\n",
        "* ### CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
        "* ### Country: Country name. Nominal, the name of the country where each customer resides."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "\n",
        "<b>**Customer segmentation is the practice of dividing a company's customers into groups that reflect similarity among customers in each group based on their shared behaviour or other attributes. Customer segmentation has the potential to allow marketers to address each customer in the most effective way. Using the large amount of data available on customers (and potential customers), a customer segmentation analysis allows marketers to identify discrete groups of customers with a high degree of accuracy based on demographic, behavioral and other indicators.The goal of segmenting customers is to decide how to relate to customers in each segment in order to maximize the value of each customer to the business. The groups should be homogeneous within them and should also be heterogeneous to each other. The main goal is to identify customers that are most profitable and loyal and the ones who churned out, to prevent further loss of customers by redefining company policies. Having a large number of customers, each with different needs it is difficult to find which customer is most important for business and target them with an appropriate strategy.**<b>\n",
        "\n",
        "**The content of the notebook includes :-**\n",
        "\n",
        "* **Data Preparation**\n",
        "* **Data Preprocessing**\n",
        "* **Feature Engineering**\n",
        "* **Exploratory Data Analysis**\n",
        "* **Model Building**\n",
        "* **Conclusion**"
      ],
      "metadata": {
        "id": "zu-JUApGqFVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Preparation**"
      ],
      "metadata": {
        "id": "9_gXZk5wqLBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1. Importing The Libraries and The Dataset**"
      ],
      "metadata": {
        "id": "6FROu5rRqUnU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dByMsuzT8Tnw"
      },
      "source": [
        "# Importing Required Libraries for our analysis\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For plots and visualizations\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from mpl_toolkits import mplot3d\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "\n",
        "# To scale the data\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# Different Clustering Algorithms\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "\n",
        "# To get optimal number of clusters in hierarchical clustering\n",
        "\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Matrics to evaluate the clusters\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "\n",
        "# To avoid unnecessary warnings, let's import warnings also\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2. Let's Mount The Drive and Import The Dataset**"
      ],
      "metadata": {
        "id": "hKzlyV2Rthhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oin5PBIhteyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define url path of the dataset\n",
        "\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/Online Retail Customer Segmentation - SUMIT KUMAR DHIBAR/Online Retail.xlsx'\n",
        "\n",
        "# Now let's import dataset to crate a dataframe\n",
        "\n",
        "cust_df = pd.read_excel(path)"
      ],
      "metadata": {
        "id": "jiwov5ajt-wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3. Understanding the Dataset**"
      ],
      "metadata": {
        "id": "yL4pofr1u2jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the shape of the dataframe\n",
        "\n",
        "print('Shape of the dataset is :', cust_df.shape)"
      ],
      "metadata": {
        "id": "-2-leBNawV6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **In our dataset we have 541909 rows and 8 columns.**\n",
        "\n"
      ],
      "metadata": {
        "id": "apxCiFyhx0Xq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking all the columns present in our dataset\n",
        "\n",
        "cust_df.columns"
      ],
      "metadata": {
        "id": "DBsf6Lqqwobl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking top 5 records in DataFrame\n",
        "\n",
        "cust_df.head()"
      ],
      "metadata": {
        "id": "7MrPW5Rm2Ub4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking bottom 5 records in DataFrame\n",
        "\n",
        "cust_df.tail()"
      ],
      "metadata": {
        "id": "Fny_bWxA2hZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the datatypes and null values of dataset\n",
        "\n",
        "cust_df.info()"
      ],
      "metadata": {
        "id": "3u2BBaMi2rH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **As it can be see from above that CustomerId, Description columns are having null values.**"
      ],
      "metadata": {
        "id": "Mg9wQFbO3TvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the statistical description of the dataset\n",
        "\n",
        "cust_df.describe().transpose()"
      ],
      "metadata": {
        "id": "t7ol1IhR3A3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b>4. Data Description </b>\n",
        "\n",
        "### <b>Attribute Information : </b>\n",
        "\n",
        "*  **InvoiceNo:** Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
        "*  **StockCode:** Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
        "*  **Description:** Product (item) name. Nominal.\n",
        "*  **Quantity:** The quantities of each product (item) per transaction. Numeric.\n",
        "*  **InvoiceDate:** Invoice Date and time. Numeric, the day and time when each transaction was generated.\n",
        "*  **UnitPrice:** Unit price. Numeric, Product price per unit in sterling.\n",
        "*  **CustomerID:** Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
        "*  **Country:** Country name. Nominal, the name of the country where each customer resides."
      ],
      "metadata": {
        "id": "yEP9Ygyy4RtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Preprocessing**"
      ],
      "metadata": {
        "id": "hKmV2E9o4YEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1. Missing Value Analysis**"
      ],
      "metadata": {
        "id": "aD-L1elQ4dVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Real world data often has a lot of missing values. The cause of missing values can be failure to record data. The handling of missing data is very important during the preprocessing of the dataset as many of the machine learnng algorithms do not support missing values.**"
      ],
      "metadata": {
        "id": "wlsmMXml4g11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the total missing data\n",
        "\n",
        "cust_df.isnull().sum()"
      ],
      "metadata": {
        "id": "11xG5UUB382M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **It can be seen from above that there are missing data in our dataset.**"
      ],
      "metadata": {
        "id": "zaz0EcW341HX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values in our dataset with the help of heatmap.\n",
        "\n",
        "plt.figure(figsize = (12,6))\n",
        "sns.heatmap(cust_df.isnull(), cmap = 'YlGnBu')\n",
        "plt.title('Heatmap for Missing Values in every column', weight = 'bold', fontsize = 14)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "7SI2fp1v4iyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage of missing data per category\n",
        "\n",
        "Total_missing_data = cust_df.isnull().sum().sort_values(ascending = False)\n",
        "Total_percentage = (cust_df.isnull().sum() / cust_df.isnull().count()).sort_values(ascending = False)*100\n",
        "missing_data_concat = pd.concat([Total_missing_data, round((Total_percentage),2)], axis = 1, keys = [\"Total No of Missing Values\", \"Percentage of Missing Values\"])\n",
        "missing_data = missing_data_concat[missing_data_concat['Total No of Missing Values']>0]\n",
        "missing_data"
      ],
      "metadata": {
        "id": "NF14Ycns6E4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **It is clear from above that CustomerID has the highest percentage of missing values followed by Description column.**"
      ],
      "metadata": {
        "id": "5Lj3wwha9OlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize the percentage of missing values\n",
        "\n",
        "plt.figure(figsize = (13,8))\n",
        "sns.set(style = 'darkgrid')\n",
        "eval = sns.barplot(x = missing_data.index, y = missing_data['Percentage of Missing Values'], data = missing_data)\n",
        "plt.title('Percentage of Missing Data', weight = 'bold')\n",
        "plt.xlabel('Features', fontsize = 14, weight = 'bold')\n",
        "plt.ylabel('Percentage of Missing Values', fontsize = 14, weight = 'bold')\n",
        "\n",
        "for i in eval.patches:\n",
        "   eval.annotate(f'{i.get_height()}', (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
        "\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "ropy9Lc48c8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we can see from above 'CustomerID' has the highest percentage of missing values followed by 'Description' column - these features are having missing values. We will treat them as we go ahead in our analysis.**"
      ],
      "metadata": {
        "id": "HlnBx_qOCVGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Total Number of Null Values\n",
        "\n",
        "print(\"Total number of null values in the data set : \", cust_df.isna().sum().sum())\n",
        "\n",
        "# Let's count all the rows which contain missing values\n",
        "\n",
        "count = 0\n",
        "for i in cust_df.isna().sum(axis = 1):\n",
        "  if i>0:\n",
        "    count = count + 1\n",
        "print('Total number of rows with missing values is ', count)\n",
        "print(f'Percentage of rows which are having missing values in the entire dataset : {round((count/len(cust_df.index))*100, 2)}%')"
      ],
      "metadata": {
        "id": "QdDASM3yA64G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As it is not possible to impute CustomerID column with some value, we will not be able to form any clusters with those missing CustomerID. We will drop the missing values from the dataset in our further analysis.**"
      ],
      "metadata": {
        "id": "cbNNhAromhDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Handling Missing Data**"
      ],
      "metadata": {
        "id": "wcm_Fg6BmnBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's first check the total number of unique values in every feature of the dataset\n",
        "\n",
        "for i in cust_df.columns.tolist():\n",
        "  print(f\"Total number of unique values in '{i}' is : \", cust_df[i].nunique())"
      ],
      "metadata": {
        "id": "s4UsIsNtk5ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's drop the rows having null values\n",
        "\n",
        "cust_df.dropna(subset = ['CustomerID'], inplace = True)\n"
      ],
      "metadata": {
        "id": "iFU9beren1ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the shape of the dataset after removing all the rows having null values\n",
        "\n",
        "print('Shape of the refreshed dataset is :', cust_df.shape)"
      ],
      "metadata": {
        "id": "X2aa-6kNoZfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the total missing data, if there is any\n",
        "\n",
        "cust_df.isnull().sum()"
      ],
      "metadata": {
        "id": "s7H31b33o5qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hurray! As it can be seen from above that we have successfully handled all the missing values in the dataset.**"
      ],
      "metadata": {
        "id": "cpMgqBilpRJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3. Handling Duplicate Data**"
      ],
      "metadata": {
        "id": "yw0tlqohpUts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It is important to remove duplicate records from the dataset, duplicate records may come from wrong collection of the data, it will add extra weight to our dataset and also increase time of the training.**"
      ],
      "metadata": {
        "id": "eZaKjvMIpZn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check if any duplicate is present\n",
        "\n",
        "print('Total number of duplicate rows :', cust_df.duplicated().sum())"
      ],
      "metadata": {
        "id": "6cqWKhERpLOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Duplicate record gives extra weight to the dataset, it is better to remove them\n",
        "\n",
        "cust_df.drop_duplicates(inplace = True)\n",
        "\n",
        "# Let's check the shape of the dataset after removing all the duplicate values\n",
        "\n",
        "print('Shape of the dataset after removing duplicate is :', cust_df.shape)"
      ],
      "metadata": {
        "id": "KfEKgH7Cp6i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Let's change the datatype of the columns** "
      ],
      "metadata": {
        "id": "PsPjmAhyrIO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's change the datatype of 'Quantity', 'UnitPrice' and 'CustomerID' column to the Integer\n",
        "\n",
        "cust_df['Quantity'] = cust_df['Quantity'].astype(int)\n",
        "\n",
        "cust_df['UnitPrice'] = cust_df['UnitPrice'].astype(int)\n",
        "\n",
        "cust_df['CustomerID'] = cust_df['CustomerID'].astype(int)"
      ],
      "metadata": {
        "id": "9uQ7Ia2tq2VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Let's check why 'Invoice No' column is having object datatype.**"
      ],
      "metadata": {
        "id": "rsbThMtatJpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's workout on the object column\n",
        "\n",
        "print('Value count for feature : InvoiceNo')\n",
        "print('\\n')\n",
        "print(cust_df['InvoiceNo'].value_counts())"
      ],
      "metadata": {
        "id": "WOEUwFS2tFct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check 'Invoice No' which contains letter 'C'\n",
        "\n",
        "cust_df['InvoiceNo'] = cust_df['InvoiceNo'].astype(str)\n",
        "\n",
        "cust_df[cust_df['InvoiceNo'].str.contains('C')]"
      ],
      "metadata": {
        "id": "hq0v9_kdt0zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As it is clear from the above observation that 'InvoiceNo' column is having 'C' at begining , which indicates 'Cancellation'. We will remove those cancelled 'InvoiceNo' as we go ahead in our analysis.**"
      ],
      "metadata": {
        "id": "3RWfeGjovNKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's drop those records from 'InvoiceNo' column, which contains 'C' in the begining\n",
        "\n",
        "cust_df = cust_df[~ cust_df['InvoiceNo'].str.contains('C')]\n",
        "\n",
        "# Let's check the shape of the dataset after removing all the Cancellation rows\n",
        "\n",
        "print('Shape of the dataset after removing Cancellation is :', cust_df.shape)"
      ],
      "metadata": {
        "id": "OXHNqGkLvB0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's change the datatype of 'InvoiceNo' column to the Integer\n",
        "\n",
        "cust_df['InvoiceNo'] = pd.to_numeric(cust_df['InvoiceNo'])\n",
        "\n",
        "cust_df['InvoiceNo'] = cust_df['InvoiceNo'].astype(int)"
      ],
      "metadata": {
        "id": "mF6qBzjIwgHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the info of the dataset after doing operations on the datatypes\n",
        "\n",
        "cust_df.info()"
      ],
      "metadata": {
        "id": "Yj07Xtl2yQ7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes! so we have converted 'InvoiceNo', 'Quantity', UnitPrice', 'CustomerID' columns to integer datatype.**"
      ],
      "metadata": {
        "id": "4h3WL2fVy5sP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature Engineering**"
      ],
      "metadata": {
        "id": "ZIeUYErgy9rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new features from the 'InvoiceDate' column\n",
        "\n",
        "cust_df['year'] = cust_df['InvoiceDate'].apply(lambda x: x.year)\n",
        "cust_df['month'] = cust_df['InvoiceDate'].apply(lambda x: x.month_name())\n",
        "cust_df['day'] = cust_df['InvoiceDate'].apply(lambda x: x.day_name())\n",
        "cust_df['hour'] = cust_df['InvoiceDate'].apply(lambda x: x.hour)"
      ],
      "metadata": {
        "id": "XuQgvDiSyv7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new feature 'Total Amount' from 'unit Price' and 'Quantity' column\n",
        "# 'Total Amount' = 'Unit Price' * 'Quantity'\n",
        "\n",
        "cust_df['Total Amount'] = cust_df['Quantity'] * cust_df['UnitPrice']"
      ],
      "metadata": {
        "id": "tH3lfme211-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new feature 'Time type' based on hours to define whether its 'morning' , 'Afternoon' or 'Evening'\n",
        "\n",
        "def time_type(time):\n",
        "  if (time > 5 and time < 12):\n",
        "    return 'Morning'\n",
        "  elif(time > 11 and time < 18):\n",
        "    return 'Afternoon'\n",
        "  else:\n",
        "    return 'Evening'  "
      ],
      "metadata": {
        "id": "MAF5rAY11BGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cust_df['Time_type'] = cust_df['hour'].apply(time_type)"
      ],
      "metadata": {
        "id": "3wzv5JuJAYZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the dataset\n",
        "\n",
        "cust_df.head()"
      ],
      "metadata": {
        "id": "fyakkuPyAoVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes! we have successfully created 6 new columns ['year', 'month', 'day', 'hour', 'Total Amount', 'Time Type'] from the dataset.**"
      ],
      "metadata": {
        "id": "MFB7cpyaFhzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Exploratory Data Analysis**"
      ],
      "metadata": {
        "id": "lD8xSkDTFmSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why do we perform EDA ?**\n",
        "\n",
        "▶  An EDA is a thorough examination meant to uncover the underlying structure of a data set and is important for a company because it exposes trends, patterns and relationships that are not readily apparent. "
      ],
      "metadata": {
        "id": "5ZC25eiaGaWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Univariate analysis**"
      ],
      "metadata": {
        "id": "M_0kiDxjGe1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Let's plot top 10 values based on frequency of each column**"
      ],
      "metadata": {
        "id": "tUDv7vxGGjvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First thing first - Let's define our target columns required to be analysed\n",
        "\n",
        "column = ['InvoiceNo', 'StockCode', 'Quantity', 'Description', 'UnitPrice', 'CustomerID', 'Country', 'year', 'month', 'day', 'hour',\n",
        "          'Total Amount', 'Time_type']\n",
        "\n",
        "# Plotting countplots for different columns of our dataset\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "plt.figure(figsize = (18,30), dpi = 90)\n",
        "for index, col in enumerate (column):\n",
        "  plt.subplot(7,2,index+1)\n",
        "  counts = cust_df[col].value_counts().reset_index().head(10)\n",
        "  counts.rename(columns = {'index' : col, col : 'count'}, inplace = True)\n",
        "  eval = sns.barplot(x= col, y = 'count', data = counts)\n",
        "  plt.xticks(rotation = 20, ha = 'right')\n",
        "  plt.title(f'Countplot of {col.title()}', weight = 'bold')\n",
        "  plt.tight_layout()\n",
        "  for i in eval.patches:\n",
        "    eval.annotate(f'{i.get_height()}', (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "r4KEYEf2Flfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Let's plot bottom 10 values based on frequency of each column**"
      ],
      "metadata": {
        "id": "D4clNGJNtdeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First thing first - Let's define our target columns required to be analysed\n",
        "\n",
        "b_column = ['StockCode' ,'Description','Quantity', 'CustomerID', 'Country']\n",
        "\n",
        "# Plotting countplots for different columns of our dataset\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "plt.figure(figsize = (15,15), dpi = 90)\n",
        "for index, col in enumerate (b_column):\n",
        "  plt.subplot(3,2,index+1)\n",
        "  counts = cust_df[col].value_counts().reset_index().tail(10)\n",
        "  counts.rename(columns = {'index' : col, col : 'count'}, inplace = True)\n",
        "  eval =  sns.barplot(x = col, y = 'count', data = counts)\n",
        "  plt.xticks(rotation = 20, ha = 'right')\n",
        "  plt.title (f'Countplot of {col.title()}', weight = 'bold')\n",
        "  plt.tight_layout()\n",
        "  for i in eval.patches:\n",
        "     eval.annotate(f'{i.get_height()}', (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L9vl0TT3FYjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Observations drawn from the above Univariate Analysis are as follows:**\n",
        "\n",
        "* **'WHITE HANGING HEART T-LIGHT HOLDER' (Stock Code - 85123A ), 'REGENCY CAKESTAND 3 TIER' (Stock Code - 22423) are the top 2 most ordered products.**\n",
        "* **Most customers are from 'United Kingdom' also considerable number of customers are also from 'Germany' , 'France', 'Eire' and 'Spain'. Whereas 'Saudi Arabia', 'Bahrain', 'Czech Republic', 'Brazil' and 'Lithuania' has least number of customers.**\n",
        "* **Most of the customer have purchased items in the month of 'October', 'November', 'December' the reason may be most of the festivals are in these months. Less number of customers have purchased the items in the month of 'January', 'February', 'April'.**\n",
        "* **There are no orders placed on 'Saturdays', the reason maybe all retail shop stay closed on this day.**\n",
        "* **Most of the customers have purchased the items in Afternoon, moderate number of customer have purchased the items in Morning and least number of customers in Evening.**"
      ],
      "metadata": {
        "id": "3fLLSLz94XKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Let's analyse Distributions of 'Total Amount' , 'Quantity' and 'Unit Price' columns.**"
      ],
      "metadata": {
        "id": "ySb5mSHS4cDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Things to keep in mind :** \n",
        "\n",
        "* **Positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer, which means mean>median>mode**\n",
        "\n",
        "\n",
        "* **Negatively skewed (also known as left-skewed) distribution is a type of distribution in which more values are concentrated on the right side (tail) of the distribution graph while the left tail of the distribution graph is longer,which means mean<median<mode**\n",
        "\n",
        "* **For symmetric graph mean=median=mode**"
      ],
      "metadata": {
        "id": "FqLJa87W4jXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot the description of each of the columns\n",
        "\n",
        "dis_col = ['Total Amount', 'Quantity', 'UnitPrice']\n",
        "\n",
        "for i in dis_col:\n",
        "  print('\\n')\n",
        "  print(f'Statistical Description of the feature : {i}')\n",
        "  print('--'*25)\n",
        "  print(cust_df[i].describe([0.75, 0.90, 0.95, 0.99]))"
      ],
      "metadata": {
        "id": "2e47NnsQ3x3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As it can be seen from above that only 1 % of values in all 3 columns has the maximum price or Quantity range, it can be said that the distribution plots will be very much skewed.**"
      ],
      "metadata": {
        "id": "RHeLnz1K9OCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizating the distributions of different columns\n",
        "\n",
        "plt.figure(figsize = (20,5))\n",
        "for n,col in enumerate (dis_col) :\n",
        "  plt.subplot(1,3,n+1)\n",
        "  sns.distplot(cust_df[col])\n",
        "  plt.title(col.title(), weight = 'bold')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "QGRaY10_86vM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Observations drawn from the above Distribution plot analysis are as follows:**\n",
        "\n",
        "* **All the plots shows very skewed( Positively skewed ) distribution because most of the values are clustered around the left side of the distribution while the right tail of the ditribution are longer, which means mean > median > mode.**"
      ],
      "metadata": {
        "id": "xy_qwWW-DN7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why we use Log Transformation ?**\n",
        "\n",
        "▶  When our original continuous data does not follow the bell curve, we can use log transformation on this data to make it as 'normal' as possible, so the analysis result from the data becomes more valid."
      ],
      "metadata": {
        "id": "C7fhLl1oDSSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's apply log transformation on skewed clumns\n",
        "\n",
        "for column in dis_col :\n",
        "  fig, axes = plt.subplots(1, 2, figsize= (16,5) )\n",
        "  sns.distplot(cust_df[column], ax = axes[0], color = 'blue').set(title = 'Before')\n",
        "  try :\n",
        "     sns.distplot(np.log(cust_df[column]), ax = axes[1], color = 'blue').set(title = 'After')\n",
        "  except :   \n",
        "     sns.distplot(np.log1p(cust_df[column]), ax = axes[1], color = 'blue').set(title = 'After')\n",
        "  plt.suptitle(f'{column.title()}',weight='bold')\n",
        "  plt.tight_layout()\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "-_Tn4I_a-IU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It can be seen from above that after applying log transformation the distribution plot looks comparatively better than being skewed.**"
      ],
      "metadata": {
        "id": "FD5lj0TpG1Fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Bivariate analysis**"
      ],
      "metadata": {
        "id": "RWJLCQY0G3qw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1. Let's analyse top 5 countries with the most customer.**"
      ],
      "metadata": {
        "id": "xY2BNST-G7qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's see the dataframe of top 5 Countries with most customers\n",
        "\n",
        "country_cust = cust_df.groupby('Country')['CustomerID'].nunique().reset_index().sort_values('CustomerID', ascending = False)\n",
        "country_cust.rename(columns = {'CustomerID' : 'Customer_count'}, inplace = True)\n",
        "country_cust.head()"
      ],
      "metadata": {
        "id": "mXE24kB5FQsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot a barplot to visualize the top 5 countries with most number of customer\n",
        "\n",
        "plt.figure(figsize = (15,5))\n",
        "sns.barplot(x = 'Country', y = 'Customer_count', data = country_cust.head(5), edgecolor = 'blue')\n",
        "plt.title('Countries with most number of customers', weight = 'bold', fontsize = 15)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "8gt440FGIv_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It is clear from above that 'United Kingdom' has most number of customers than any other countries.**"
      ],
      "metadata": {
        "id": "Rz37EdvHKSmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2. Let's analyse top 5 countries with the most orders placed.**"
      ],
      "metadata": {
        "id": "aPhymcoKKVNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's see the dataframe of top 5 Countries with most orders placed\n",
        "\n",
        "country_ord = cust_df.groupby('Country')['InvoiceNo'].nunique().reset_index().sort_values('InvoiceNo', ascending = False)\n",
        "country_ord.rename(columns = {'InvoiceNo' : 'Orders'}, inplace = True)\n",
        "country_ord.head()"
      ],
      "metadata": {
        "id": "21FlERkRKN4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot a bar plot to visualize the top 5 countries with most orders placed\n",
        "\n",
        "plt.figure(figsize = (15,5))\n",
        "sns.barplot(x = 'Country', y = 'Orders', data = country_ord.head(5), edgecolor = 'blue')\n",
        "plt.title ('Countries with highest orders' , weight = 'bold', fontsize = 15)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "C2sHJnDhLkSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**'United kingdom' here also topped with most order placed compared to other countries.**"
      ],
      "metadata": {
        "id": "JpQ8GgIc8kvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3. Let's analyse top 5 countries with orders in mass quantity.**"
      ],
      "metadata": {
        "id": "jQrTgNxP8oaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's see the dataframe of top 5 countries with orders in mass quantity\n",
        "\n",
        "country_quant= cust_df.groupby('Country')['Quantity'].mean().reset_index().sort_values('Quantity', ascending = False)\n",
        "country_quant.rename(columns = {'Quantity' : 'Avg_quant'}, inplace = True)\n",
        "country_quant.head()"
      ],
      "metadata": {
        "id": "f-ErSTTv8r0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot a bar plot to visualize the top 5 countries with most orders placed\n",
        "\n",
        "plt.figure(figsize = (15,5))\n",
        "sns.barplot(x = 'Country', y = 'Avg_quant', data = country_quant.head(5), edgecolor = 'blue')\n",
        "plt.title ('Countries with mass quantity orders placed' , weight = 'bold', fontsize = 15)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "5mqxiyN3NRBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It is clear from above that orders with mass quantity placed by the customer from Netherlands.**"
      ],
      "metadata": {
        "id": "hhzKwGKlRVft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4. Let's analyse top 5 items with most purchased according to quantity.**"
      ],
      "metadata": {
        "id": "XWuJKgjZRYmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's see the dataframe of top 5 items with most purchesed according to quantity\n",
        "\n",
        "desc_quant = cust_df.groupby('Description')['Quantity'].sum().reset_index().sort_values('Quantity', ascending = False)\n",
        "desc_quant.head()"
      ],
      "metadata": {
        "id": "HWFqOkb1Rakm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot a bar plot to visualize the top 5 items with most purchesed according to quantity\n",
        "\n",
        "plt.figure(figsize = (15,5))\n",
        "sns.barplot(x = 'Description', y = 'Quantity', data = desc_quant.head(5), edgecolor = 'blue')\n",
        "plt.title('Items with most orders', weight = 'bold', fontsize = 15)\n",
        "plt.xticks(rotation = 20, ha = 'right')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "DlB548gGUt1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From above plot it can be stated that 'PAPER CRAFT , LITTLE BIRDIE' , 'MEDIUM CERAMIC TOP STORAGE JAR'  these are the top 2 items with most purchased in quantity.**\t\t"
      ],
      "metadata": {
        "id": "PRrmAhvzW3Al"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**5. Let's analyse top 5 items with highest total amount.**"
      ],
      "metadata": {
        "id": "Jhc89AJDW6Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's see the dataframe of top 5 items with highest total amount.\n",
        "\n",
        "desc_amount = cust_df.groupby('Description')['Total Amount'].sum().reset_index().sort_values('Total Amount', ascending = False)\n",
        "desc_amount.head()"
      ],
      "metadata": {
        "id": "I9id0vlMWqsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot a bar plot to visualize the top 5 items with highest total amount.\n",
        "\n",
        "plt.figure(figsize = (15,5))\n",
        "sns.barplot(x = 'Description', y = 'Total Amount', data = desc_amount.head(5), edgecolor = 'blue')\n",
        "plt.title('Items with highest revenue', weight  = 'bold', fontsize = 15)\n",
        "plt.xticks(rotation = 20, ha = 'right')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "LcXxqmXEXwZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It can be stated that 'PAPER CRAFT , LITTLE BIRDIE' product has made highest revenue.**\t"
      ],
      "metadata": {
        "id": "84b0_aFtZIFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**6. Let's analyse top 5 items purchased by most customer.**"
      ],
      "metadata": {
        "id": "-E7Ten1qZL87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's see the dataframe of top 5 items purchased by most customer.\n",
        "\n",
        "item_customer = cust_df.groupby('Description')['CustomerID'].nunique().reset_index().sort_values('CustomerID', ascending = False)\n",
        "item_customer.rename(columns = {'CustomerID' : 'Customer_count'}, inplace = True)\n",
        "item_customer.head()"
      ],
      "metadata": {
        "id": "gxPcDapbY8mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot a bar plot to visualize the top 5 items purchased by most customer.\n",
        "\n",
        "plt.figure(figsize = (15,5))\n",
        "sns.barplot(x = 'Description', y = 'Customer_count', data = item_customer.head(5), edgecolor = 'blue')\n",
        "plt.title('Items with most customers', weight = 'bold', fontsize = 15)\n",
        "plt.xticks(rotation = 20, ha = 'right')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "TXKJ2b5oamZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Item 'REGENCY CAKESTAND 3 TIER'\tis the choice of most of the customer.**"
      ],
      "metadata": {
        "id": "L8M4LNvdcQHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model Building**"
      ],
      "metadata": {
        "id": "ZLxVawnPcTTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**RFM MODEL**"
      ],
      "metadata": {
        "id": "yJT1xzRjcW3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before applying any clustering algorithms it is always necessary to determine various quantitative factors on which the algorithm will perform segmentation. Examples of these would be features such as total amount spend by the particular customer, how frequent the customer visit the shop or what is the last visit of the customer.**\n",
        "\n",
        "###**What is RFM ?**\n",
        "\n",
        "**RFM Model which stands for Recency, Frequency and Monetary is one of such steps in which we determine the followings.**\n",
        "\n",
        "* **Recency : How recently did the customer visit our website or how recently did the customer purchase.** \n",
        "\n",
        "* **Frequency : How often do they visit or how often do they purchase.**\n",
        "\n",
        "* **Monetary : How much revenue we get from their visit or how much do they spend when they purchase.**\n",
        "\n",
        "**However, these approach does not require machine learning algorithms as segmentation can be done manually.**"
      ],
      "metadata": {
        "id": "3SAHBvPWccf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Why it is needed ?**\n",
        "\n",
        "**RFM analysis is a marketing framework that is used to understand and analyze customer behaviour based on the above three factors Recency, Frequency and Monetary.**\n",
        "\n",
        "**The RFM analysis will help the businesses to segment their customer base into different homogenous groups so that they  can engage with each group with different targeted marketing strategies.**"
      ],
      "metadata": {
        "id": "8GmhQP92ch__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's first make a copy of the original dataset on which the model building will be done.\n",
        "\n",
        "customer_df = cust_df.copy()"
      ],
      "metadata": {
        "id": "1TPcAJqScKoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Formula ➡**\n",
        "\n",
        "* **Recency = Latest Date - Last Inovice Date.**\n",
        "* **Frequency = count of invoice no. of transaction(s).** \n",
        "* **Monetary = Sum of Total Amount for each customer.**"
      ],
      "metadata": {
        "id": "rwilzXU9c_tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_df.columns"
      ],
      "metadata": {
        "id": "Xsjwnmkvc68Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "\n",
        "# First let's set latest date 2011-12-10 as last invoice date was 2011-12-09. This is to calculate the number of days from recent purchase\n",
        "\n",
        "Latest_Date  = dt.datetime(2011,12,10)\n",
        "\n",
        "# Create RFM Modelling scors for each customer\n",
        "\n",
        "rfm_df = customer_df.groupby('CustomerID').agg({'InvoiceDate': lambda x: (Latest_Date - x.max()).days,\n",
        "                                                'InvoiceNo': lambda x: len(x),\n",
        "                                                'Total Amount': lambda x: x.sum()})\n",
        "\n",
        "# Convert Invoice Date into type int\n",
        "\n",
        "rfm_df['InvoiceDate'] = rfm_df['InvoiceDate'].astype(int)\n",
        "\n",
        "# Rename column names to Recency, Frequency and Monetary\n",
        "\n",
        "rfm_df.rename(columns={'InvoiceDate': 'Recency',\n",
        "                       'InvoiceNo': 'Frequency',\n",
        "                       'Total Amount': 'Monetary'}, inplace = True)\n",
        "\n",
        "rfm_df.reset_index().head()"
      ],
      "metadata": {
        "id": "laDz20dedGTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of above dataframe :**\n",
        "\n",
        "1. Recency : How recent a customer made a purchase.\n",
        "2. Frequency : How often a customer makes a purchase.\n",
        "3. Monetary : How much money a customer spends in total."
      ],
      "metadata": {
        "id": "LD5hcusRhcoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating R,F & M scores by splitting Recency, Frequency & Monetary based on quantities\n",
        "\n",
        "rfm_df['R'] = pd.qcut(rfm_df['Recency'], q = 4, labels = [1,2,3,4]).astype(int)\n",
        "rfm_df['F'] = pd.qcut(rfm_df['Frequency'], q = 4, labels = [4,3,2,1]).astype(int)\n",
        "rfm_df['M'] = pd.qcut(rfm_df['Monetary'], q = 4, labels = [4,3,2,1]).astype(int)\n",
        "\n",
        "# Finding the RFM group for each customer by combining the factors R, F & M\n",
        "\n",
        "rfm_df['RFM_Group'] = 100*rfm_df['R'] + 10*rfm_df['F'] + rfm_df['M']\n",
        "\n",
        "# Finding RFM score for each customer by adding the factors R, F & M\n",
        "\n",
        "rfm_df['RFM_Score'] = rfm_df['R'] + rfm_df['F'] + rfm_df['M']"
      ],
      "metadata": {
        "id": "MWmyPmTPh9Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the dataframe afterr adding different columns\n",
        "\n",
        "rfm_df.head()"
      ],
      "metadata": {
        "id": "-a4Pspi_hMw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of above dataframe :**\n",
        "* If the R,F & M of any customer is 111, then we can say that the customers Recency is very good that means customer visited store very recently, Frequency is also very good for the resepective customer that means number of time the customer visited store is very high and also Monetary is very good for the customer which means the customer spend considerably high amount during his/her total visit at store.\n",
        "\n",
        "* If the R,F & M of any customer is 444, then we can say that the customers Recency is very bad that means customer visited store while back, Frequency is also very bad for the resepective customer that means number of time the customer visited store is very less and also Monetary is very bad for the customer which means the customer spend very low amount during his/her total visit at store.\n",
        "\n",
        "* If the R,F & M of any customer is 411, then we can say that the customer purchased long time ago but buys frequently and spends more\n",
        "\n",
        "Like that we can come up with number of segments for all combinations of R,F & M. Lower the RFM score more valuable the customer is."
      ],
      "metadata": {
        "id": "wK4hVJJDlJDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's understand the Descriptive Statistics of different columns in the above dataset\n",
        "\n",
        "rfm_col = ['Recency', 'Frequency', 'Monetary']\n",
        "\n",
        "for i in rfm_col:\n",
        "  print('\\n')\n",
        "  print(f'Statistical Description of the feature : {i}')\n",
        "  print('--'*25)\n",
        "  print(rfm_df[i].describe([0.75, 0.90, 0.95, 0.99]))"
      ],
      "metadata": {
        "id": "Tv5QPgfKhzU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As it is clear from above that 'min' value of all the 3 columns are 0, which will give error in the transformations, we will treat them in further analysis.**"
      ],
      "metadata": {
        "id": "N0AjUozZmdP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling zeros in the dataframe to avid error in the transformations\n",
        "\n",
        "# Let's define a function to treat the zeros of the above dataframe\n",
        "\n",
        "def zero_removal(num):\n",
        "  if (num == 0):\n",
        "    return 1\n",
        "  else:\n",
        "    return num\n",
        "\n",
        "# Let's apply 'zero_removal' function to 'Recncy', 'Frequency' & 'Monetary' columns\n",
        "\n",
        "rfm_df['Recency'] = [zero_removal(x) for x in rfm_df['Recency']]\n",
        "rfm_df['Frequency'] = [zero_removal(x) for x in rfm_df['Frequency']]\n",
        "rfm_df['Monetary'] = [zero_removal(x) for x in rfm_df['Monetary']]"
      ],
      "metadata": {
        "id": "8S-c1JQYmJoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the distributions of different columns\n",
        "\n",
        "plt.figure(figsize = (20,5))\n",
        "for n,col in enumerate (rfm_col) :\n",
        "  plt.subplot(1,3,n+1)\n",
        "  sns.distplot(rfm_df[col])\n",
        "  plt.title(col.title(), weight = 'bold')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "yK9z3dp1ox8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It can be seen from above that 'Recency', 'Frequency' & 'Monetary' columns are very much rightly skewed (Positive skewed), we will treat them by applying log transformation.**"
      ],
      "metadata": {
        "id": "iL0kUel5qMRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Log Transformation on columns for smoothing the distribution\n",
        "\n",
        "rfm_df['Recency_log'] = rfm_df['Recency'].apply(np.log)\n",
        "rfm_df['Frequency_log'] = rfm_df['Frequency'].apply(np.log)\n",
        "rfm_df['Monetary_log'] = rfm_df['Monetary'].apply(np.log)"
      ],
      "metadata": {
        "id": "7oj3OjAdpuuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the dataframe after adding new columns\n",
        "\n",
        "rfm_df.head()"
      ],
      "metadata": {
        "id": "3abDDttprMD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Descriptive Statistics of the Dataframe\n",
        "\n",
        "rfm_df.describe()"
      ],
      "metadata": {
        "id": "IOvnjlrNraHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the disatributions before and after the log transformation\n",
        "\n",
        "target_column = ['Recency', 'Frequency', 'Monetary']\n",
        "\n",
        "for column in target_column :\n",
        "  fig, axes = plt.subplots(1, 2, figsize= (16,5) )\n",
        "  sns.distplot(rfm_df[column], ax = axes[0], color = 'blue').set(title = 'Before')\n",
        "  sns.distplot(np.log(rfm_df[column]), ax = axes[1], color = 'blue').set(title = 'After')\n",
        "  plt.suptitle(f'{column.title()}',weight='bold')\n",
        "  plt.tight_layout()\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "jYraDUPersTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "* Earlier the distributions of Recency, Frequency and Monetary columns were positively skewed but after applying log transformation the distributions appear to be symmetrical and normally distributed.\n",
        "\n",
        "* It will be more suitable to use the transformed features for better visualization of clusters."
      ],
      "metadata": {
        "id": "13m1w2T5ubaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Visualizing the correlations among RFM features.**"
      ],
      "metadata": {
        "id": "5ng_JwXdugHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfm_coll = [\"RFM_Group\", \"RFM_Score\", \"Recency_log\", \"Frequency_log\", \"Monetary_log\"]\n",
        "\n",
        "plt.figure(figsize = (10,6), dpi = 110)\n",
        "sns.heatmap(abs(rfm_df[rfm_coll].corr()), annot = True, cmap = 'coolwarm')\n",
        "plt.title('RFM Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oP82CPPKt-94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ody8F8VtvjsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **It is clear from above that 'Recency' is highly correlated with RFM_Group value , whereas Frequency and Monetary is moderately correlated with RFM_Group.**\n",
        "* **Also RFM_Score is equally correlated with 'Recency',  'Frequency' and 'Monetary'.**\n",
        "\n",
        "* **Note :**\n",
        "        * RFM_Group : It is the concatination of R,F and M scores.\n",
        "        * RFM_Score : It is the summation of R,F & M scores."
      ],
      "metadata": {
        "id": "7LhRPFAhv4OM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Scaling**"
      ],
      "metadata": {
        "id": "S0StqS7pwC4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Define X Variable\n",
        "\n",
        "X = rfm_df[['Recency_log', 'Frequency_log', 'Monetary_log']]"
      ],
      "metadata": {
        "id": "pxAXfNpPwGNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Scale the variables with the help of StandardScaler method.\n",
        "\n",
        "X = StandardScaler().fit_transform(X)"
      ],
      "metadata": {
        "id": "wsHLLCS8wgWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ":* **Mean and Standard Deviation of values in the RFM features after scaling are 0 and 1 respectively.**"
      ],
      "metadata": {
        "id": "MYsiz3ZHyqHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Clustering**"
      ],
      "metadata": {
        "id": "sKcu-03Zyy90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the result in results variable\n",
        "\n",
        "results = { }\n",
        "\n",
        "# Let's define a function to remove the outliers\n",
        "\n",
        "def treating_outliers(col):\n",
        "  '''\n",
        "      This above function takes a column and removes the outliers\n",
        "      col : The respective column which is to be cleaned\n",
        "  '''\n",
        "  col_q1 = col.quantile(0.25)\n",
        "  col_q3 = col.quantile(0.75)\n",
        "  col_iqr = col_q3 - col_q1\n",
        "  condition_1 = (col >= col_q1 - 1.5 * col_iqr)\n",
        "  condition_2 = (col <= col_q3 + 1.5 * col_iqr)\n",
        "  return col[condition_1 & condition_2]\n",
        "\n",
        "# Now let's define a function for displaying the mean and median of Recency, Frequency and Monetary for each group of customers\n",
        "\n",
        "def cluster_stats(dataset, segment):\n",
        "  '''\n",
        "    This above function returns the mean, median and count of Recency, Frequency and Monetary for each group of customers\n",
        "  '''\n",
        "  statistic = dataset[['Recency', 'Frequency', 'Monetary', segment]].groupby(segment).agg(['mean', 'median'])\n",
        "  statistic['count'] = dataset[segment].value_counts()\n",
        "\n",
        "# Let's store 0.25th and 0.75th quantile of Recency, Frequency & Monetary for each group of customers\n",
        "\n",
        "result = dataset[['Recency', 'Frequency', 'Monetary', segment]].groupby(segment).agg({\n",
        "                'Recency': lambda x : f'{int(x.quantile(0.25))} to {int(x.quantile(0.75))} days ago',\n",
        "                'Frequency' : lambda x : f'Bought {int(x.quantile(0.25))} to {int(x.quantile(0.75))} times',\n",
        "                'Monetary' :  lambda x : f'Spend around {int(x.quantile(0.25))} to {int(x.quantine(0.75))} pound Sterling',\n",
        "                })\n",
        "\n",
        "# Let's rename the columns\n",
        "statistic.columns = [f\"{i}  {j}\" for i, j in statistic.columns]\n",
        "result.columns = ['Last visited', 'Purchase frequency', 'Amount spent']\n",
        "\n",
        "return statistic, result\n",
        "\n",
        "\n",
        "# Now let's define function for plottting clusters for visualization\n",
        "\n",
        "def plot_statistic_result(y, segment, title):\n",
        "  '''\n",
        "    This above function plots the clusters for the given data and returns the dataframes for the clusters with con"
      ],
      "metadata": {
        "id": "hYzc5ku8w-Yq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}